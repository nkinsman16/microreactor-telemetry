# Micro-Reactor Telemetry Data System

An end-to-end data system that simulates micro-reactor telemetry,
processes it in real time with Apache Flink, and stores results in
Delta Lake for analysis with Spark SQL.

# Data source

Source: UC Irvine Machine Learning Repository
https://www.kaggle.com/datasets/rinichristy/combined-cycle-power-plant-data-set-uci-data?resource=download

Features consist of hourly average ambient variables

Temperature (T) in the range 1.81°C and 37.11°C,
Ambient Pressure (AP) in the range 992.89-1033.30 milibar,
Relative Humidity (RH) in the range 25.56% to 100.16%,
Exhaust Vacuum (V) in teh range 25.36-81.56 cm Hg,
Net hourly electrical energy output (EP) 420.26-495.76 MW

# System Architecture

The system moves data through four main components:

1. A Python-based simulator generates micro-reactor telemetry at a chosen emission rate.
2. Apache Flink reads the incoming stream, performs simple checks on each record, and applies the expected schema.
3. Validated records are written out as Delta-style JSON files in the output directory.
4. Spark SQL loads these JSON files to run queries and analyze the stored telemetry.

# Repository Structure

The project is organized into the following parts:

- simulator/ – Python code that generates the telemetry stream and includes the dataset.
- flink/ – Flink job that reads the stream, applies basic checks, and writes the output.
- spark_sql/ – Notebook containing example Spark SQL queries on the JSON files.
- output/ – Directory where the validated records are written.
- src/ – Additional Java source files used by the Flink pipeline.
- pom.xml – Maven build file for compiling and running the Flink job.

# Running the Simulator

The telemetry stream is generated by the Python script in the simulator/ directory.  
From the project root, it can be started with:

python simulator/generator.py

The emission rate and other small settings can be adjusted inside the script if needed.

# Running the Flink Job

The Flink pipeline reads the simulator output, validates each record, and writes JSON files to the output/ directory.

To run it:

1. Start a local Flink cluster.
2. Build the project with: mvn clean package
3. Submit the job using the generated JAR in the target/ directory.

Once the job is running, new telemetry from the simulator will appear in output/reactor_json/. The spark_sql directory contains an initial notebook that was intended for loading the JSON output and running Spark SQL queries. The full querying workflow was not completed, but the structure for starting a SparkSession is in place.
